{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual o objetivo do comando cache em Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O cache do Spark pode armazenar o resultado de quaisquer dados de subconsulta e dados armazenados em formatos diferentes (como CSV, JSON, parquet e ORC) em RDD. Isto permite melhorar a eficiência do código pois possibilita que resultados intermediários de operações lazy (operações que não são executadas quando são chamadas, mas somente quando uma operação de Ação é chamada ou disparada) possam ser armazenados e reutilizados repetidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No MapReduce uma nova instância JVM é iniciada para cada job executado e o processamento dos dados ocorre em etapas que partem da leitura dos dados até a escrita do resultado dos mesmos em disco (gerando os dados intermediários) para que estes sejam utilizados na próxima operação (job) (leitura – processamento operação 1 – escrita operação 1 – leitura resultado operação 1 – processamento operação 2 – escrita operação 2, ...)\n",
    "Já o Spark opera a partir de todo o conjunto de dados sendo os resultados intermediários de todas as operações passados diretamente entre elas através do caching dos mesmos em memória (leitura – processamento – escrita). Além disto, o Spark mantém a JVM constantemente em execução em cada um dos nós sendo necessário apenas iniciar uma nova thread. \n",
    "Porém, vale ressaltar que devem ser avaliadas todas as variáveis existentes por exemplo algoritimo a ser implementado, memória RAM disponível e propósito do aplicação e decidir qual framework é mais adequado ao objetivo esperado e ao custo envolvido tendo em vista que operações em MapReduce são indicadas para movimentação batch de grandes volumes de dados enquanto o Spark tem uma implementação mais ampla ao permitir o processamento de grandes volumes de dados tanto em Batch quanto em streaming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual é a função do SparkContext?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função serve para criar uma conexão com um cluster Spark e ativar serviços internos do ambiente Spark com o auxilio do Resource Manager configurado que aloca os recursos através das aplicações conforme configurações/parâmetros estabelecidos. Ele estabelece configurações de memória e processamento de worknodes. Pode ser utilizado, com o auxílio do Resource Manager, para criação de RDD, acumuladores e publicar variáveis nos clusters além de possibilitar obtenção do status atual, definir configurações e disparar/cancelar um job por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explique com suas palavras o que é Resilient Distributed Datasets (RDD)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD é a estrutura de dados do SPARK caracterizada por possibilitar a distribuição (particionamento) de dados de maneira segura em diferentes nós em um cluster e executar operações com estes sendo esta distribuição possível devido à característica de resiliência uma vez que, possibilita que os dados na memória que por ventura venham a ser perdidos por possíveis falhas possam ser recriados da mesma forma como foram originados (imutáveis).\n",
    "Para garantir estas características os RDDs suportam dois tipos de operações: transformação e ação. A operação de transformação garante a resiliência e imutabilidade dos dados uma vez que ao serem recriados dados perdidos os mesmos são recriados em novas RDDs sem que as RDDs “originais”sejam modificadas.  \n",
    "Toda vez que existir a necessidade de output de dados é executada uma operação de Ação, desta forma, uma ação de transformação não é iniciada sem que uma operação de ação seja solicitada. (Lazy Evaluation conforme descrito na primeira questão)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey é menos eficiente que reduceByKey em grandes datasets. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto o GroupByKey agrupa os dados com base na chave num processo de redução único no RDD redutor (reducer), o reduceByKey realiza o agrupamento pela chave localmente no RDD mapper usando as funções associativa e comutativa antes de enviar os dados ao RDD redutor de forma similar ao método “combiner” do MapReduce.\n",
    "O GroupByKey transfere todo Dataset pela rede, enquanto o reduceByBey calcula somas locais para cada chave em cada partição e combina essas somas locais em somas maiores após o shuffle de forma que um conjunto substancialmente menor de dados e movimentado consumindo menos recursos de memoria e rede.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explique o que o código Scala abaixo faz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Picture1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Picture2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega um arquivo de texto de um determinado diretorio do HDFS para a variavel textFile num RDD usando contexto Spark\n",
    "\n",
    "val textFile = sc.textFile(\"hdfs://...\")\n",
    "\n",
    "### Quebra cada linha em uma sequencia de palavras usando o criterio de espaco em branco e armazena o conjunto sequencial de palavras na variavel counts (novo RDD) e em seguida executa as duas próximas instruções abaixo\n",
    "\n",
    "val counts = textFile.flatMap(line.split(\"\"))\n",
    "\n",
    "### Transforma cada palavra criando uma relação de chave-valor entre a palavra e o número 1\n",
    "\n",
    "        .map(word => (word, 1))\n",
    "    \n",
    "### Gera agrupamento por palavras de forma a somar a quantidade de incidências \n",
    "\n",
    "        .reduceByKey(_+_)\n",
    "    \n",
    "### Grava o resultado com a contagem de palavras do RDD criado em formato texto no HDFS    \n",
    "\n",
    "counts.saveASTextFile(\"hdfs://...\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questões - datasets NASA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets:\n",
    "### \tJulho -> ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz\n",
    "### \tAgosto -> ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patoolib\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Usuario\\\\Desktop\\\\Desafio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz\", \n",
    "    \"C:\\\\Users\\\\Usuario\\\\Desktop\\\\Desafio\\\\Jul.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz \", \n",
    "    \"C:\\\\Users\\\\Usuario\\\\Desktop\\\\Desafio\\\\Ago.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patoolib.extract_archive(\"Ago.gz\", outdir=\"C:\\\\Users\\\\Usuario\\\\Desktop\\\\Desafio\")\n",
    "\n",
    "patoolib.extract_archive(\"Jul.gz\", outdir=\"C:\\\\Users\\\\Usuario\\\\Desktop\\\\Desafio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "jul = sc.textFile('access_log_Jul95',use_unicode=True)\n",
    "jul = jul.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "ago = sc.textFile('access_log_Aug95',use_unicode=True)\n",
    "ago = ago.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Número de hosts únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de hosts únicos em Julho: 56\n",
      "Número de hosts únicos em Agosto: 54\n"
     ]
    }
   ],
   "source": [
    "# Número de hosts únicos\n",
    "jul_count = jul.flatMap(lambda line: str(line.encode('utf-8')).split(' ')[0]).distinct().count()\n",
    "ago_count = ago.flatMap(lambda line: str(line.encode('utf-8')).split(' ')[0]).distinct().count()\n",
    "print('Número de hosts únicos em Julho: %s' % jul_count)\n",
    "print('Número de hosts únicos em Agosto: %s' % ago_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tO total de erros 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de erros 404\n",
    "def response_code_404(line):\n",
    "    try:\n",
    "        code = line.split(' ')[-2]\n",
    "        if code == '404':\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de erros 404 em Julho: 10.845\n",
      "Total de erros 404 em Agosto: 10.056\n"
     ]
    }
   ],
   "source": [
    "jul_404 = jul.filter(response_code_404).cache()\n",
    "ago_404 = ago.filter(lambda line: str(line.encode('utf-8')).split(' ')[-2] == '404').cache()\n",
    "\n",
    "print('Total de erros 404 em Julho: %s' % format(jul_404.count(), \",\").replace(\",\", \".\"))\n",
    "print('Total de erros 404 em Agosto: %s' % format(ago_404.count(), \",\").replace(\",\", \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tOs 5 URLs que mais causaram erro 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julho: \n",
      "\n",
      "Top 5 URLs que mais causaram erro 404: \n",
      "\n",
      "/pub/winvn/readme.txt 667\n",
      "/pub/winvn/release.txt 547\n",
      "/history/apollo/apollo-13.html 286\n",
      "/shuttle/resources/orbiters/atlantis.gif 232\n",
      "/history/apollo/a-001/a-001-patch-small.gif 230\n",
      "\n",
      "Agosto: \n",
      "\n",
      "Top 5 URLs que mais causaram erro 404: \n",
      "\n",
      "/pub/winvn/readme.txt 1337\n",
      "/pub/winvn/release.txt 1185\n",
      "/shuttle/missions/STS-69/mission-STS-69.html 683\n",
      "/images/nasa-logo.gif 319\n",
      "/shuttle/missions/sts-68/ksc-upclose.gif 253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/pub/winvn/readme.txt', 1337),\n",
       " ('/pub/winvn/release.txt', 1185),\n",
       " ('/shuttle/missions/STS-69/mission-STS-69.html', 683),\n",
       " ('/images/nasa-logo.gif', 319),\n",
       " ('/shuttle/missions/sts-68/ksc-upclose.gif', 253)]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 URLs que mais causaram erro 404\n",
    "def top5_endpoints(rdd):\n",
    "    endpoints = rdd.map(lambda line: str(line.encode('utf-8')).split('\"')[1].split(' ')[1])\n",
    "    counts = endpoints.map(lambda endpoint: (endpoint, 1)).reduceByKey(add)\n",
    "    top = counts.sortBy(lambda pair: -pair[1]).take(5)\n",
    "    \n",
    "    print('\\nTop 5 URLs que mais causaram erro 404: \\n')\n",
    "    for endpoint, count in top:\n",
    "        print(endpoint, count)\n",
    "\n",
    "    return top\n",
    "\n",
    "print(\"Julho: \")\n",
    "top5_endpoints(july_404)\n",
    "print(\"\\nAgosto: \")\n",
    "top5_endpoints(august_404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tQuantidade de erros 404 por dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de erros 404 retornados no mês de Julho: \n",
      "\n",
      "13/Jul/1995 532\n",
      "21/Jul/1995 334\n",
      "25/Jul/1995 461\n",
      "09/Jul/1995 348\n",
      "15/Jul/1995 254\n",
      "16/Jul/1995 257\n",
      "18/Jul/1995 465\n",
      "17/Jul/1995 406\n",
      "07/Jul/1995 570\n",
      "12/Jul/1995 471\n",
      "19/Jul/1995 639\n",
      "22/Jul/1995 192\n",
      "23/Jul/1995 233\n",
      "03/Jul/1995 474\n",
      "05/Jul/1995 497\n",
      "10/Jul/1995 398\n",
      "14/Jul/1995 413\n",
      "01/Jul/1995 316\n",
      "02/Jul/1995 291\n",
      "04/Jul/1995 359\n",
      "06/Jul/1995 640\n",
      "08/Jul/1995 302\n",
      "11/Jul/1995 471\n",
      "20/Jul/1995 428\n",
      "24/Jul/1995 328\n",
      "26/Jul/1995 336\n",
      "27/Jul/1995 336\n",
      "28/Jul/1995 94\n",
      "\n",
      "Total de erros 404 retornados no mês de Agosto: \n",
      "\n",
      "01/Aug/1995 243\n",
      "07/Aug/1995 537\n",
      "09/Aug/1995 279\n",
      "10/Aug/1995 315\n",
      "21/Aug/1995 305\n",
      "27/Aug/1995 370\n",
      "30/Aug/1995 571\n",
      "03/Aug/1995 304\n",
      "06/Aug/1995 373\n",
      "08/Aug/1995 391\n",
      "16/Aug/1995 259\n",
      "20/Aug/1995 312\n",
      "05/Aug/1995 236\n",
      "11/Aug/1995 263\n",
      "12/Aug/1995 196\n",
      "13/Aug/1995 216\n",
      "15/Aug/1995 327\n",
      "17/Aug/1995 271\n",
      "22/Aug/1995 288\n",
      "23/Aug/1995 345\n",
      "24/Aug/1995 420\n",
      "26/Aug/1995 366\n",
      "28/Aug/1995 410\n",
      "14/Aug/1995 287\n",
      "18/Aug/1995 256\n",
      "19/Aug/1995 209\n",
      "31/Aug/1995 526\n",
      "04/Aug/1995 346\n",
      "25/Aug/1995 415\n",
      "29/Aug/1995 420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('01/Aug/1995', 243),\n",
       " ('07/Aug/1995', 537),\n",
       " ('09/Aug/1995', 279),\n",
       " ('10/Aug/1995', 315),\n",
       " ('21/Aug/1995', 305),\n",
       " ('27/Aug/1995', 370),\n",
       " ('30/Aug/1995', 571),\n",
       " ('03/Aug/1995', 304),\n",
       " ('06/Aug/1995', 373),\n",
       " ('08/Aug/1995', 391),\n",
       " ('16/Aug/1995', 259),\n",
       " ('20/Aug/1995', 312),\n",
       " ('05/Aug/1995', 236),\n",
       " ('11/Aug/1995', 263),\n",
       " ('12/Aug/1995', 196),\n",
       " ('13/Aug/1995', 216),\n",
       " ('15/Aug/1995', 327),\n",
       " ('17/Aug/1995', 271),\n",
       " ('22/Aug/1995', 288),\n",
       " ('23/Aug/1995', 345),\n",
       " ('24/Aug/1995', 420),\n",
       " ('26/Aug/1995', 366),\n",
       " ('28/Aug/1995', 410),\n",
       " ('14/Aug/1995', 287),\n",
       " ('18/Aug/1995', 256),\n",
       " ('19/Aug/1995', 209),\n",
       " ('31/Aug/1995', 526),\n",
       " ('04/Aug/1995', 346),\n",
       " ('25/Aug/1995', 415),\n",
       " ('29/Aug/1995', 420)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total de erros 404 diários\n",
    "def daily_count(rdd):\n",
    "    days = rdd.map(lambda line: line.split('[')[1].split(':')[0])\n",
    "    counts = days.map(lambda day: (day, 1)).reduceByKey(add).collect()\n",
    "    \n",
    "\n",
    "    for day, count in counts:\n",
    "        print(day, count)\n",
    "        \n",
    "    return counts\n",
    "\n",
    "print('\\nTotal de erros 404 retornados no mês de Julho: \\n')\n",
    "daily_count(jul_404)\n",
    "print('\\nTotal de erros 404 retornados no mês de Agosto: \\n')\n",
    "daily_count(ago_404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tO total de bytes retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(B):   \n",
    "   B = float(B)\n",
    "   KB = float(1024)\n",
    "   MB = float(KB ** 2) # 1,048,576\n",
    "   GB = float(KB ** 3) # 1,073,741,824\n",
    "   TB = float(KB ** 4) # 1,099,511,627,776\n",
    "\n",
    "   if B < KB:\n",
    "      return '{0} {1}'.format(B,'Bytes' if 0 == B > 1 else 'Byte')\n",
    "   elif KB <= B < MB:\n",
    "      return '{0:.2f} KB'.format(B/KB)\n",
    "   elif MB <= B < GB:\n",
    "      return '{0:.2f} MB'.format(B/MB)\n",
    "   elif GB <= B < TB:\n",
    "      return '{0:.2f} GB'.format(B/GB)\n",
    "   elif TB <= B:\n",
    "      return '{0:.2f} TB'.format(B/TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de bytes retornados em Julho: 36.04 GB\n",
      "Total de bytes retornados em Agosto: 24.99 GB\n"
     ]
    }
   ],
   "source": [
    "# Total de bytes retornados\n",
    "def accumulated_byte_count(rdd):\n",
    "    def byte_count(line):\n",
    "        try:\n",
    "            count = int(line.split(\" \")[-1])\n",
    "            if count < 0:\n",
    "                raise ValueError()\n",
    "            return count\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    count = rdd.map(byte_count).reduce(add)\n",
    "    return count\n",
    "\n",
    "\n",
    "print('Total de bytes retornados em Julho: %s' % format_size(accumulated_byte_count(jul)))\n",
    "print('Total de bytes retornados em Agosto: %s' % format_size(accumulated_byte_count(ago)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
